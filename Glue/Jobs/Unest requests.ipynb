{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %timeout            Int           The number of minutes after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session.\n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n    %reconnect          String        Specify a live session ID to switch/reconnect to the sessions.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n    %matplot      Matplotlib figure   Visualize your data using the matplotlib library.\n                                      E.g. \n                                      import matplotlib.pyplot as plt\n                                      # Set X-axis and Y-axis values\n                                      x = [5, 2, 8, 4, 9]\n                                      y = [10, 4, 8, 5, 2]\n                                      # Create a bar chart \n                                      plt.bar(x, y) \n                                      # Show the plot\n                                      %matplot plt    \n    %plotly            Plotly figure  Visualize your data using the plotly library.\n                                      E.g.\n                                      import plotly.express as px\n                                      #Create a graphical figure\n                                      fig = px.line(x=[\"a\",\"b\",\"c\"], y=[1,3,2], title=\"sample figure\")\n                                      #Show the figure\n                                      %plotly fig\n\n  \n                \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nimport json\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.types import StructType, ArrayType\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: 1238c3ab-279d-4d75-bb06-560cbfc1799d\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session 1238c3ab-279d-4d75-bb06-560cbfc1799d to get into ready status...\nSession 1238c3ab-279d-4d75-bb06-560cbfc1799d has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Definimos la función que necesitaremos usar para formatear correctamente los archivos raw.\n\nYa que el método explode solo funciona con elementos tipo Array, debemos crear una función que aplane elementos tipo Struct.\n\n* La función recibirá el schema del dataframe, iteraremos cada campo y mediante recursión iremos desanidando todos aquellos campos de tipo estructura.\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Definimos la función para aplanar las StructTypes que encontremos en el DataFrame\ndef flatten(schema, prefix=None):\n    fields=[]\n    for field in schema.fields:\n        name=f\"{prefix}.{field.name}\" if prefix else f\"{field.name}\"\n        dtype=field.dataType\n        if isinstance(dtype, StructType):\n            fields+=flatten(dtype, prefix=name)\n        else:\n            fields.append(name)\n    return fields",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# El plan a seguir es:\n\n* Cargar los archivos con los que vamos a trabajar. \n* Aplanar los dataframes. \n* Unir los datos.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "bucket_name=\"s3://bucket-for-requests/data/api_database/requests_json/\"\ninput_files=[\"housing_data.json\", \"housing_data_2.json\", \"housing_data_3.json\"]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dataframes=[]\nfor input_file in input_files:\n    input_path=f\"{bucket_name}{input_file}\"\n    df=spark.read.json(input_path)\n    dataframes.append(df)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "explode_dataframes=[]\nfor dataframe in dataframes:\n    dataframe=dataframe.select(explode(\"elementList\").alias(\"flattened_elementList\"))\n    explode_dataframes.append(dataframe)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "flattened_dataframes=[]\nfor dataframe in explode_dataframes:\n    flattened_df=dataframe.select(flatten(dataframe.schema))\n    flattened_dataframes.append(flattened_df)\n\nflattened_df1,flattened_df2,flattened_df3=flattened_dataframes",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "count=1\nfor df in flattened_dataframes: # Realizamos el conteo de filas para chequear la correcta implementación de los union. En total deberían ser 7450 filas\n    rows=df.count()\n    print(f\"El dataframe {count} tiene {rows} filas0\")\n    count+=1",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "El dataframe 1 tiene 2500 filas\nEl dataframe 2 tiene 2950 filas\nEl dataframe 3 tiene 2000 filas\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": " # Realizamos las operaciones union pertinentes para trabajar con un único dataframe.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "first_union_df=flattened_df1.union(flattened_df2)\nprint(f\"El dataframe contiene {first_union_df.count()} filas\")\nlast_union_df=first_union_df.union(flattened_df3)\nprint(f\"El dataframe contiene {last_union_df.count()} filas\") # Observamos que el conteo de filas es correcto",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "El dataframe contiene 5450 filas\nEl dataframe contiene 7450 filas\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "last_union_df.printSchema()# Observamos la estructura final del archivo",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- address: string (nullable = true)\n |-- bathrooms: long (nullable = true)\n |-- country: string (nullable = true)\n |-- description: string (nullable = true)\n |-- subTypology: string (nullable = true)\n |-- typology: string (nullable = true)\n |-- distance: string (nullable = true)\n |-- district: string (nullable = true)\n |-- exterior: boolean (nullable = true)\n |-- externalReference: string (nullable = true)\n |-- floor: string (nullable = true)\n |-- has360: boolean (nullable = true)\n |-- has3DTour: boolean (nullable = true)\n |-- hasLift: boolean (nullable = true)\n |-- hasPlan: boolean (nullable = true)\n |-- hasStaging: boolean (nullable = true)\n |-- hasVideo: boolean (nullable = true)\n |-- groupDescription: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- municipality: string (nullable = true)\n |-- neighborhood: string (nullable = true)\n |-- newDevelopment: boolean (nullable = true)\n |-- newDevelopmentFinished: boolean (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- operation: string (nullable = true)\n |-- hasParkingSpace: boolean (nullable = true)\n |-- isParkingSpaceIncludedInPrice: boolean (nullable = true)\n |-- parkingSpacePrice: double (nullable = true)\n |-- price: double (nullable = true)\n |-- priceByArea: double (nullable = true)\n |-- amount: double (nullable = true)\n |-- currencySuffix: string (nullable = true)\n |-- formerPrice: double (nullable = true)\n |-- priceDropPercentage: long (nullable = true)\n |-- priceDropValue: long (nullable = true)\n |-- propertyCode: string (nullable = true)\n |-- propertyType: string (nullable = true)\n |-- province: string (nullable = true)\n |-- rooms: long (nullable = true)\n |-- showAddress: boolean (nullable = true)\n |-- size: double (nullable = true)\n |-- status: string (nullable = true)\n |-- subtitle: string (nullable = true)\n |-- title: string (nullable = true)\n |-- thumbnail: string (nullable = true)\n |-- topNewDevelopment: boolean (nullable = true)\n |-- topPlus: boolean (nullable = true)\n |-- url: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_flattened=DynamicFrame.fromDF(last_union_df, glueContext, \"dyf_flattened\") # Convertimos el DataFrame a un DynamicFrame\ndyf_flattened.printSchema()\ndyf_flattened.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- address: string\n|-- bathrooms: long\n|-- country: string\n|-- description: string\n|-- subTypology: string\n|-- typology: string\n|-- distance: string\n|-- district: string\n|-- exterior: boolean\n|-- externalReference: string\n|-- floor: string\n|-- has360: boolean\n|-- has3DTour: boolean\n|-- hasLift: boolean\n|-- hasPlan: boolean\n|-- hasStaging: boolean\n|-- hasVideo: boolean\n|-- groupDescription: string\n|-- latitude: double\n|-- longitude: double\n|-- municipality: string\n|-- neighborhood: string\n|-- newDevelopment: boolean\n|-- newDevelopmentFinished: boolean\n|-- numPhotos: long\n|-- operation: string\n|-- hasParkingSpace: boolean\n|-- isParkingSpaceIncludedInPrice: boolean\n|-- parkingSpacePrice: double\n|-- price: double\n|-- priceByArea: double\n|-- amount: double\n|-- currencySuffix: string\n|-- formerPrice: double\n|-- priceDropPercentage: long\n|-- priceDropValue: long\n|-- propertyCode: string\n|-- propertyType: string\n|-- province: string\n|-- rooms: long\n|-- showAddress: boolean\n|-- size: double\n|-- status: string\n|-- subtitle: string\n|-- title: string\n|-- thumbnail: string\n|-- topNewDevelopment: boolean\n|-- topPlus: boolean\n|-- url: string\n\n{\"address\": \"SAN SINFORIANO, 4\", \"bathrooms\": 2, \"country\": \"es\", \"description\": \"PUEDES TENER UNA OPORTUNIDAD ÚNICA DE REALIZAR UNA GRAN INVERSIÓN, PARA VIVIR, SI ES TU PRIMERA VIVIENDA, O PARA OBTENER UNA GRAN RENTABILIDAD MEDIANTE EL ALQUILER, EN EL BARRIO DE BARAJAS, A ESCASOS MINUTOS DEL AEROPUERTO Y DE LA Stella Homes Barajas es un edificio muy especial de 17 viviendas de 1 dormitorio y estudios en sus plantas tipo, y áticos dúplex con terraza de 2 dormitorios, EN EL QUE PODRÁS ELEGIR ENTRE VIVIR O INVERTIR. Apartamentos en un edificio residencial de nueva construcción de diseño moderno, dotado de un gran nivel de calidad y UNA ETIQUETA ENERGÉTICA A, en pleno casco histórico de BARAJAS, a escasos 150 metros de la Avda. de Logroño, y a 5 minutos andando de la estación de metro de Barajas (línea 8). Barajas, es un barrio de Madrid, en continua renovación de edificios y creciente expansión, muy tranquilo y con poco tráfico, totalmente consolidado con gran oferta de servicios (parques, colegios, ambulatorio, restaurantes, comercios de proximidad, …) y una comunicación perfecta, por transporte público con metro y numerosas líneas de autobús, a unos minutos de las terminales 1,2 y 3 del aeropuerto, y fácil acceso en coche a la A2, M-40, M-11, M-14 y M-12.  La promoción, propone una memoria de calidades selecta, destacando entre otras, sus instalaciones de climatización y agua caliente producidas mediante aerotermia centralizada de alta eficiencia, calefacción y refrigeración por suelo radiante y suelo refrescante, y apoyo de paneles fotovoltaicos para la instalación eléctrica de sus zonas comunes, puertas de paso con entrecalles horizontales, y armarios equipados con cajoneras; suelos de porcelánico imitación madera y pintura lisa plástica; baño con mueble encimera y espejo, y ducha extraplana de resina con mampara y grifería termostática, así como la cocina amueblada con frigorífico, entre otras. Stella Homes Barajas, es una promoción que está orientada a clientes que empiezan a crecer y buscan independizarse y conseguir el acceso a su primera vivienda, pero también es ideal para aquellos clientes que su motivación de compra es la inversión, bien para que tus hijos puedan utilizarla o bien pudiéndote ofrecer una estupenda rentabilidad mediante el alquiler. La licencia de obras está solicitada, previendo el inicio de las obras para el tercer trimestre de este año 2024. Su entrega se prevé para el tercer trimestre de 2026 Si la promoción es de tu interés, puedes concertar una visita a nuestras oficinas centrales situadas en la calle Alcalá, 96, 5º-Dcha. Mediante cita previa. *Las viviendas de planta ático, no disponen de trastero.\", \"subTypology\": \"duplex\", \"typology\": \"flat\", \"distance\": \"13515\", \"district\": \"Barajas\", \"exterior\": true, \"externalReference\": \"3º E\", \"floor\": \"3\", \"has360\": false, \"has3DTour\": false, \"hasLift\": true, \"hasPlan\": true, \"hasStaging\": false, \"hasVideo\": false, \"groupDescription\": null, \"latitude\": 40.4731153, \"longitude\": -3.5806433, \"municipality\": \"Madrid\", \"neighborhood\": \"Casco Histórico de Barajas\", \"newDevelopment\": true, \"newDevelopmentFinished\": false, \"numPhotos\": 12, \"operation\": \"sale\", \"hasParkingSpace\": true, \"isParkingSpaceIncludedInPrice\": true, \"parkingSpacePrice\": null, \"price\": 409000.0, \"priceByArea\": 4648.0, \"amount\": 409000.0, \"currencySuffix\": \"€\", \"formerPrice\": null, \"priceDropPercentage\": null, \"priceDropValue\": null, \"propertyCode\": \"105224655\", \"propertyType\": \"duplex\", \"province\": \"Madrid\", \"rooms\": 2, \"showAddress\": true, \"size\": 88.0, \"status\": \"newdevelopment\", \"subtitle\": \"Casco Histórico de Barajas, Madrid\", \"title\": \"Dúplex en San Sinforiano, 4\", \"thumbnail\": \"https://img4.idealista.com/blur/WEB_LISTING/0/id.pro.es.image.master/fd/b0/a3/1245695846.webp\", \"topNewDevelopment\": false, \"topPlus\": false, \"url\": \"https://www.idealista.com/obra-nueva/105224655/\"}\n{\"address\": \"barrio Sol\", \"bathrooms\": 2, \"country\": \"es\", \"description\": \"ÚNICA Inmobiliaria pone a su disposición este magnífico piso exterior, recién reformado, en una de las calles principales del barrio de Cortes-Huertas. El inmueble se encuentra situado en una cuarta planta con una espectacular reforma realizada con materiales de primera calidad. La vivienda se compone de salón independiente, cocina de diseño, dos dormitorios, uno de ellos en suite, y dos baños.  Espectacular edificio con fachada protegida. Es una finca tranquila, debido al escaso número de vecinos.  La zona es una de las más demandadas de la capital, puesto que está cerca de enclaves como la Puerta del Sol o la Gran Vía, repleta de comercios, de ocio, restauración y de centros educativos.  Perfecta comunicación de trasporte por numerosas líneas de autobuses y metro.  En Única Inmobiliaria aglutinamos más de veinte años de experiencia en el sector del lujo. Contamos con una amplia trayectoria que avala nuestra gestión trabajando alrededor de las zonas más exclusivas de Madrid: Barrio de Salamanca, Recoletos, Almagro, Chamartín, Retiro, Justicia… NUESTRA CARTERA DE PROPIEDADES INCLUYE OTROS INMUEBLES EN ESTA MISMA ZONA QUE NO APARECEN EN IDEALISTA POR EXPRESA VOLUNTAD DE SUS PROPIETARIOS. CONSULTE NUESTRA WEB: unicainmobiliaria. com.\", \"subTypology\": null, \"typology\": \"flat\", \"distance\": \"2712\", \"district\": \"Centro\", \"exterior\": true, \"externalReference\": \"cm-425891\", \"floor\": \"4\", \"has360\": false, \"has3DTour\": false, \"hasLift\": true, \"hasPlan\": true, \"hasStaging\": false, \"hasVideo\": false, \"groupDescription\": null, \"latitude\": 40.4164512, \"longitude\": -3.7025562, \"municipality\": \"Madrid\", \"neighborhood\": \"Sol\", \"newDevelopment\": false, \"newDevelopmentFinished\": null, \"numPhotos\": 21, \"operation\": \"sale\", \"hasParkingSpace\": null, \"isParkingSpaceIncludedInPrice\": null, \"parkingSpacePrice\": null, \"price\": 789000.0, \"priceByArea\": 7812.0, \"amount\": 789000.0, \"currencySuffix\": \"€\", \"formerPrice\": null, \"priceDropPercentage\": null, \"priceDropValue\": null, \"propertyCode\": \"105156989\", \"propertyType\": \"flat\", \"province\": \"Madrid\", \"rooms\": 2, \"showAddress\": false, \"size\": 101.0, \"status\": \"good\", \"subtitle\": \"Sol, Madrid\", \"title\": \"Piso\", \"thumbnail\": \"https://img4.idealista.com/blur/WEB_LISTING/0/id.pro.es.image.master/1f/4e/05/1243526044.webp\", \"topNewDevelopment\": false, \"topPlus\": false, \"url\": \"https://www.idealista.com/inmueble/105156989/\"}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Terminamos en este apartado, guardando el archivo correctamente formateado pero RAW.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Con el dynamic frame ahora podemos escribir el archivo en formato parquet para construir el delta lake.\nglueContext.write_dynamic_frame.from_options(frame=dyf_flattened, connection_type=\"s3\", connection_options={\"path\": \"s3://bucket-for-requests/data/api_database/bronze/\"}, format=\"parquet\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 67,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fe55ac17520>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "job.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}